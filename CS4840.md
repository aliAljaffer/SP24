# Intro to Machine Learning - CS4840

Machine Learning: Lern from data for some specific task with measures

Types:

- Supervised learning
- classification, regression
- Unsupervised learning
- Sub-groups based on feature distribution: anomaly detection
- Semi-supervised

Workflow of supervised:

1. Data collection
2. feature representation (Preprocessing)
3. optimization (Training)
4. model evaluation (Test)

Linear Classification:

- Feature vector
- Weight vector
- Prediction Score
- Classification output
- Decision boundary
- Margin: $(w \cdot pred(x)) y$
- Loss function: zero-one loss.

Linear Regression: - Regression output: prediction score - Residual: (w.pred(x)-y) - Square loss - Absolute deviation loss

Gradient Descent:

- Gradient: Derivative of train loss wrt weights
- Train loss needs to be differentiable
- Steps:
  - initialize weights
  - compute gradient
  - update w <- w
  - $\eta\nabla TrainLoss(w)$
  - Learning rate
    - too small coverges to minimum slowly, too large could diverge
- Stochastic GD
  - Use a training example to compute G at each step
  - Advantage: Faster than trad. GD.
  - Disadvantage: Erratic and converge to sub-optimal solution
- Mini-batch GD

  - Use a batch of training example to compute G at each step
  - Batch size
  - Advantage: Less erratic than SGD faster than trad GD, good solution
  - Disadvantage: Need to well choose an appropriate mini-batch size
    Training and test data split

- Split your labeled data into training set for training and test set for evaluation
- The amount of training set and test set may be different but their features are the same
  Evaluation Metrics for Regression:
- Root Mean Square Error (RMSE): $\sqrt(\frac{1}{D_{test}} ∑(w \cdot pred(x)-y)^2)$
- Mean Absolute Error (MAE):

Feature Preprocessing

Cross-Entropy Loss

Confusion Matrix:
TP, FP
FN, TN

Metrics for binary classification:

- $Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$
- $Precision = \frac{TP}{TP+FP}$
- $Recall = (TP)/(TP+FN)$
- F1 Score = $\frac {2*precision*recall}{precision+recall} = \frac{TP}{TP + 0.5(FP+FN)}$

Metrics for multi-class:

- Accuracy = correct predictions/total
- Macro F1: Compute F1 score for each class, and avg the scores
- Micro F1: Compute TP,FP and FN for each class, sum up to get total TP,FP,FN and compute F1 based on totals

Cross Validation
Hold-out:
Simplest eval method
Steps:

- Split data into training test sets
- train the model using training set
- evaluate the model using test set
  K-Fold cross validation
  Stratified cross validation

K-Nearest Neighbors

L-0 Hamming

L-1 Manhattan

L-2 Euclidean

L-3 Chebyshev

Distance Metrics

How to choose K?

Neighbor search
Brute Force
K-Dimensional Tree
Ball Tree

No SVM

## Support Vector Machines

**Support Vectors** are data points that are closest to the hyperplane and impact on the position and orientation of the hyperplane

Margin - The distance between classes

MultiClass Classification

Train multiple binary classifications`

Ways for SVM to multi-class:

- One-vs-one: perform classification between each pair of classes
  - $\frac{n(n-1)}{2}$
- One-vs-rest: perform classification to seperate each class from the rest
  - $n-1 $
- variant: One-vs-All need to train n classifiers for n classes

C1 C2 C3 C4

C $2 \choose 5$

Limitations: - Issue 1: Searching for high dimensions need high comp resources and computations in high dimensions are expenseive - Issue 2: Many more parameters (weights) to be optimized

Kernel Trick of SVM

Kernel Trick: Use a kernel function to compute the dot-product in feature space and return the result in higher dimension directly

for non-linear transformation, we need to first map all the data $x_{i}$ -> $\psi(x_{i})$ and $x_{j}$ -> $\psi(x_{j})$ to higher-dimensional and compute the dot-product between new feature vectores psi(xi).dot(psi(xj))

$$K(x_{i}, x_{j}) = \psi(x_{i}) \cdot \psi(x_{j})$$

$$
x_{i} = [a_{1},a_{2}]\\
x_{j} = [b_{1},b_{2}]\\
K(x_{i},x_{j}) = (x_{i} \cdot (x_{j}))^2
$$

Kernel Functions - Polynomial: $$K(x_{i} , x_{j}) = (\gamma  x_{i} \cdot x_{j} + r)^d$$

- Gaussian - RBF - Sigmoid

Optimization Function with Kernel Trick:

$$y = f_{w}(x) = ∑_{i=0}^na_{i}y_{i}\cdot K(x_{i},x)$$

(Kernel Trick)
Advantages: - Flexible feature space: map the data points into higher dim without expensive computations - Excellent results

Disadvantages: - Must choose kernel function and the parameters - When mapping data points into very high feature dimernsions, the SVM model could be easily over-fitting

Epsilon: How “wide” the margin is

SVM can also be used for Regression

sklearn SVR

Decision Trees

Definition: A Decision Tree is a supervised learning model for classification and regression; also the fundamental components of random forest, which is one of the most powerful ML models available today.

Components of a DT:

- Internal Node: Root and leaf nodes - represent a test on a feature
- Branches: Stem from internal nodes, represent an option
- Leaf nodes: Nodes without children - represent a potential outcome for each outcome
- Path: Traced from the root to a leaf which is a full prediction

Gini index/impurity: A metric used to evaluate how pure a set of data points in the current node during the decision tree construction:

$$G = 1 - ∑^n_{k=1}p_{k}^2$$

$p_{k}$ is the ration of instances in class $k$ among the training instances in the current node.

- When $G = 0$ in a node, this data split is perfect and the node is pure with all the instances belonging to the same class
- When $G > 0$, the larger Gini score indicates worse data split and the instances in the node is more mixed with different classes.

CART Training Algorithm

Optimization problem of DT for classification: Select a feature and a feature value to split the training data points in the current node, where the Gini scores for the children nodes need to be minimized

- Step 1: First split the training data points into two subsets using a single feature $x$ and a threshold $t$ (for numerical features: petal length ≤ 2.45 cm) of a value $T$ (for categorical features: color = yellow)
- Step 2: Search for the pair of $(x, t)$ that produces the subsets with the minimal Gini scores, weighted by their sizes. CART tries to minimize the cost function
  - $$J(k,t) = (\frac{M_{left}}{M})*G_{left} + (\frac{M_{right}}{M})*G_{right}$$
- Step 3: Once a feature $k$ and threshold/value $t$ is found, and the training data points are successfully split into two, then CART splits the subsets using the same logic then the sub-subsets, and so on, recursively
- Step 4: CART stops once it reaches the maximum depth or it cannot find a split that can reduce the Gini scores

Decision Tree tries to minimize the local cost

**Entropy**: Uncertainty or randomness is another metric used to evaluate how pure a set of data points in the current node during the decision tree construction

$$H=-∑^n_{k=1} p_{k} log_{2} p_{k}$$

When $H=0$ the subset only contains instances of one class.

### Estimating Class Probabilities

- Decision tree can output the class directly, and estimate the class probability as well

- First traverse the tree to the leaf node, and then returns the ration of training data points in each class

$[0,49,5] \rightarrow $ Training points: 0 for setosa, 49 for versicolor, and 5 for virginica

As such we can get class probabilities:

$[\frac{0}{54},\frac{49}{54},\frac{5}{54}] \rightarrow  [0,0.91,0.09]$

## Regression

Regression with decision trees

- Optimization problem of decision tree for regression - select a feature and a feature value to split the training data points in the current node where the mean square error for the children nodes need to be minimized

- CART (Classification and Regression) training algo - can also be used for regression model construction

Process $\downarrow$

same process as classification, just different way to get leaf node. in leaf node for regression we return mean value of true labels of all data points in the leaf node.

1. Split the training data points into two subsets using a single feature $x$ and a threshold $t$ or a value $t$

2. Search for the pair of $(x,t)$ that produces the subsets with minimal MSE, weighted by their sizes. CART tries to minimize the cost function

$$
J(k,t) = \frac{m_{left}}{m}MSE_{left} + \frac{m_{right}}{m}MSE_{right}
$$

$$
MSE_{node} = ∑_{i\ \in\ points\ in\  node}(y_{node}-y^i)^2
$$

Advantages:

- Simple to understand and interpret
- Can perform on numerical and categorical features

Disadvantages:

- One

Solution:

- sdj

## Ensemble Learning

Suppose you need to get answer to a complex question? What is a reasonable way to do that?

You may ask this question to different random people and then aggregate their answers

This is called the wisdom of the crowd.

When training a model, combining the contributions from different features is better than from individual feature.

Aggregating the predictions of a group of machine learning models often gets better performance.

**Ensemble learning**: A general machine learning method to seek better predictive performance where multiple individual models are trained for the same problem, and the prediction result comes from the combination of all these models.

- **Homogenous** - All the individual models are the same types
- **Heterogenous** - The individual models are different types

### Why use ensemble?

- **Performance**: An ensemble can make better predictions and achieve better performance than any single model.

- **Robustness**: An ensemble reduces the possibilities of the predictions violated by attackers.

$$ a = P(x ≥ 501) = 1 - P(x < 500)$$
$$ a = 1- (P(x = 0) + P(x = 1) + ... + P(x = 499))$$

<!-- prettier-ignore -->
$$ a = 1 - ∑^{499}_{i=0} P(x = i)$$

<!-- prettier-ignore -->
$$ a = 1 - {∑}_{i=0}^{499} {1000 \choose{i}} 0.51^i0.49^{499-i} \approx 0.75$$

We start with original training data.

1. Create multiple data sets
2. Build multiple classifiers
3. Combine classifiers

   a. Regression: get mean

   b. Classification: get majority

Hard voting: Make majority vote on the predicted class directly

Soft voting: Combine all prediction probabilities

#### Bagging

An ensemble method to seek a diverse group of members by varying the training data

- Generate many dataset from the original training dataset
  - Random sampling with replacement -> bootstrap sampling -> Bagging(If we have a small dataset)
  - Random sampling without replacement -> Pasting (If we have a large dataset)
- Train an individual mode on each sampled dataset
- Make the prediction:

  - Voting for classification: The class receiving the most votes
  - Averaging for regression: Get the average

##### Out-of-Bag

The training instances that are not sampled by any base models are out-of-bag(OOB) instances. We can use OOB instances directly to evaluate the ensemble by averaging OOB evaluations of each model.

Make sure to enable bootstrap and oob_score in bagging classifier params.

#### Stacking

Short for stacked generalization: An ensemble method to seek a diverse group of members by varying the models.

The individual model is different, while the training dataset is the same for individual models.

- Divide the training dataset into two subsets, 1 and 2.
- Train diff. models(diff. algos.) on subset 1.
- Use these trained models to make predictions over subset 2: n models have n predictions.
- Use these predictions as new features to train a blender model to aggregate the predictions and output the final prediction.

#### Boosting

Individual models will be trained in sequential order, each one tries to correct the previously trained model.

Corrects previous classifier's mistakes by increasing the weight of the wrongly predicted datapoints.

#### AdaBoost (Adaptive Boosting)

- Assign a weight $W^i$ to each instance in the training dataset, which is $\frac{1}{m}$ where $m$ is the number of datapoints.
- Sample m instances, and train a model and calculate the **error rate**:

$$r = \frac{∑^m_{i=1}W^i(y'^i ≠ y^i)}{∑^m_{i=1}W^i}$$

- Calculate the prediction weight of the current model:

  $$\alpha = log\frac{1-r}{r}$$

- Update the weight for each training instance:

  $$W^i = W^i exp(\alpha)\ if\ y'^i ≠ y^i$$

  $$W^i = {W^i}/{∑^m_{i=1}}$$

  $$exp(x) = e^x$$

- Sample m instances and train a new model using the same logic and so on.

- Final prediction output over n models:

$$f(x) = sign(∑^n_{i=1}\alpha^if^i(x))$$

## Random Forests

Multiple DTs

A great quality of RFs is to make it easy to measure the feature importance

Shallow learning model: One time

## Neural Networks

| Bio | NN equivalent|
| --- | --- |
| Signals | input features x|
| Neuron | Weighted combination of features $∑^d_{j=1}w_{j}x_{j}$|
| Activation function (decide the neoron output signals) | Output = $\sigma(w\cdot x) = 0\ if\ w\cdot x<0\ else\ 1$ |
| Perceptron | WHAT |

- Multi-layer perceptron: Organizes a number of perceptrons in a layer-wise network, where all the neurons in a later are connected to every neoron in the prev. layer and the following layer, and all perceptrons in a layer have no connection. 3 layers or more.

- MLP is a feedforward artificial neural network.

**Input layer:** depends on features.

**Output layer:** The conclusions (classification/regression) of the neural networks, which are derived from all the computations performed by multi-layer perceptrons.

**Hidden layer:** The set of neurons where all the computations are performed on the input data, which is the core of the neural networks. Each hidden layer is also called fully connected layer. There can be any number of hidden layers in a neural network. The simplest network consists of a single layer.

$$h_{1} = activation(∑^5_{i=1}w_{1i}x_{i})$$
$$h_{2} = activation(∑^5_{i=1}w_{2i}x_{i})$$
$$h_{3} = activation(∑^5_{i=1}w_{3i}x_{i})$$
$$h_{4} = activation(∑^5_{i=1}w_{4i}x_{i})$$
$$h_{5} = activation(∑^5_{i=1}w_{5i}x_{i})$$

$h=\varphi(W^{(1)T}x)$

$x$ is the feature vector

$h$ is the hidden layer

$W$ is the weight matrix

$\varphi$ is the activation function

$q=\varphi(W^{(2)T}h)$

$z=\varphi(W^{(3)T}q)$

$y=\varphi(W^{(4)T}z)$

Activation function $\varphi$ playes a key role for the performance of neural networks.

### Activation Functions

A function usd by artificial neoron to generate the output.

Good quality of an activation function - Continuous, differntiable, non-linear, and can map the output into the values in the continous range.

- Sigmoid: $\phi(x) = \frac{1}{1+e^{-x}}$
  - Advantage:
    - The function is differentiable and monotonic, but the function's derivative is not, which is good for gradient calculation.
    - The function is non-linear which is good for complex tasks.
  - Disadvantage:
    - The function output ranges between 0 and 1. All neurons in the hidden layer are activated, making the NN complex and easily overfitting.

Generally,we use sigmoid func in the final output layer to compute the probabilities that predict the imput example as different classes.

Classification with one input: sigmoid

Classification with more than one input: softmax

- Hyperbolic tangent activation function: $tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}$
  - This function is similar to sigmoid, the only difference is the output range $[-1,1]$
  - Advantage and disadvantage similar to sigmoid.
- ReLU (Rectified Linear Unit): $R(z) = max(0,z)$
  - The most widely used activation function right now which is used by any type of NN
  - ReLU has all the advantages of ther functions.
  - Addresses the disadvantage of sigmoid function: some neurons will be deactivatied as zero, making the neural network lighter
- Leaky ReLU: when $\ x < 0\rightarrow y_{i}=a_{i}x_{i}$

### Backpropagation

Update parameters from the last layer to the first layer.

$W^{(4)} \leftarrow W^{(4)} - \eta \nabla TrainLoss(W^{(4)})$

$\nabla_{W^{(4)}} =\frac{\partial y}{\partial W^{(4)}}\cdot z$

$...$

$h=\varphi(W^{(1)T}x)$

$q=\varphi(W^{(2)T}h)$

$z=\varphi(W^{(3)T}q)$

$y=\varphi(W^{(4)T}z)$

### Parameters vs Hyperparameters

Parameters - Can be updated and optimized automatically during the model/neural networks optimization

Hyperparameters - Learning rate, slack, etc.

### Finding a Hyperparameter

- Grid Search

### Why Deep Neural Networks?

When the problem is simple and feature size is small - shallow learning models may be a better choice for fast and easy implementation and comparable performance to deep neural networks. (Iris classification)

When the problem is complex or feature size is very large - shallow learning models with one-time weighted combination of input features may not be able to capture the feature patterns to facilitate classification. (Facial recognition)

- Weighted combination of input features for each neuron - different from each other, which captures the input features in different perspective.

- Edge information!

### NN for Regression

$y = softmax(W^{(4)T}z+b^{(4)})$

Use activation function based on problem and range of expected values.

Side Note: Bias = Weight

## Bias and Variance

Related to model performance.

High Bias (High training error) $\rightarrow$ Underfitting: No correlation btn. input and output data.

High Variance (High test error) $\rightarrow$ Overfitting: Model is too fit to the input data where it can't predict new data outside the input.

Bias-variance trade-off: To build a NN, the goal is to fit a model to the data with a good trade-off btn. bias and variance.

Low training error and low test error is best. Leading to low bias and low variance!

**Basic Recipe for NN:**

- Does the model have high bias? Take a look at the training error rate (high) or training accuracy (low)
  - Yes $\rightarrow$ Bigger network (more hidden layers or more hidden neurons), train more epochs, add more features, change better networks. Check bias again.
  - No $\downarrow$

- Does the model have high variance?
  - Yes $\rightarrow$ More data, reduce complexity of the model to avoid overfitting, change to better networks.
  - No $\rightarrow$ Done!

## Regularization

High variance/overfitting: model significantly relies on all features in the training data and large weights in a NN are a sign of model overfitting the training data.

Regularization is a typical method to decrease the values of weights and address the overfitting problme

$$TrainLoss(W) + \lambda\lvert\lvert W\rvert\rvert^2$$

$\lambda$ - Regularization parameter. When $\approx$ 0, focus on trainloss.

## Early stopping

Set an early stop point for epochs.

Early stop: Another widely used method to address overfitting problem for NN, which forces the NN optimization (or train loss minimization to stop the earlier stage before the optimal solution)

Intuition: When using GD to min. the TrainLoss, each step makes model better fit training data; up to a point, this improves the model's performacnce on the training and test data; past that point, the training error is further decreased, but the test error would be increased.

## Dropout

Dropout: The most popular way to be used to address overfitting problem in deep neural netwworks, which is also very simple and effective.

Randomly "sleeps" neurons leading to less reliance on any single feature.

### Dropout to Avoid Overfitting

**When the NN is fully connected for each layer** - it's easy for the network to learn the co-dependency over all these features, leading to large weights and overfitting.

**When part of neurons are dropped out in each layer** - the model will learn co-dependency over random part of features, but the weights will be shared by all neurons.
For each training iter:

- Drop out a fixed portion of random neurons
- Perform the fwd propagation
- ...

## Batch normalization

## Implementing MLP/DNN

Dense layers: fully connected neurons

## Popular NN

- Images: Convolutional Neural Networks (CNN)
  - An NN which can take in an input image, learn weights to various aspects in the image through multiple laters, and extract high-level features. These features are then fed to multi-layer perceptron for classification.
    - Advantage: Compared to a flattening the image to a line of features, CNN can better capture the local spatial dependencies.
    - Advantage: CNN can significantly reduce the number of parameters (weights)
  - Input layer
  - Convolution layer - kernel application
  - Pooling layer
  - Fully-connected layer
  - Output layer
- Text/Documents: Recurrent Neural Networks (RNN)

- Graph-structured data: Graph Neural Networks (GNN)

## Quiz 9 Question

![Alt text](image.png)

Choose M1:
Left(0s): 1 C 4 NC (5 total) ($1 - 0.2^2 - 0.8^2 = 0.32$)
Right(1s): 1 C (1 total) ($1 - 1^2 = 0$)
Cost: 6 samples, $left\_ branch\_ cost = \frac{5}{6}*0.32+\frac{1}{6}*0$

Choose M2 ... M3

### Padding

Used when new channel is odd-sized. Just add 0s.

## Recurrent neural network RNN

Used for data involving sequences (time-series, sentence). (In order)

### Structure

- Input layer - given a sequence $[x_{0},x_{1},x_{2}, ...,x_{n}]$

- Hidden layer - calculate $h_{t} = \varphi (w_{x}x{t} + w_{h}h_{t-1}$ using the current $x_{t}$ and the hidden state $h_{t-1}$ from the previous hidden layer

- Fully-connected layer - Reads the outpput from the last hidden layer and perform forward propagation

- Output layer - apply softmax

## Graph NN

## Unsupervised Learning

A machine learning model groups the data into different categories using feature distrbuition without any labels to help.

- The data is trained by measuring the distance between the training samples' feature spaces

- The group of test data is generally decided by the closest distance between the test data and the samples in each group.

### Types of Unsupervised Learning

- Clustering: A task of identifying similar instances and assigning them to clusters.
  - Advantage:

- Anomaly detection: Identifies data points, events, and/or observations that deviate from a dataset's normal behavior.
  - Advantages: Detecting unusual credit card transactions to prevent fraud, catching manufacturing defects, or automatically re,oving outliers from a dataset before feeding it to another learning algorithm.

### K-Means

- K-means: One of the most popular partitioning methods, which constructs k partitions of the data, and each partition represents a cluster.
  - Each cluster: is represented by the mean value of the features from the data in the cluster.

### Density-based Clustering Method

- DBSCAN
